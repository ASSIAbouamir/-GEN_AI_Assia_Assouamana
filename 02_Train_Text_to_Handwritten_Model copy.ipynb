{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Étape Suivante : Création d'un Dataset PyTorch et Entraînement Basique d'un Modèle Text-to-Handwritten\n",
        "\n",
        "Ce notebook utilise les paires préparées dans `01_Prepare_IAM_Dataset.ipynb` pour entraîner un modèle cGAN simple. Objectifs :\n",
        "1. Charger les paires (texte tokenisé + images) à partir de `iam_lines_pairs.npz`.\n",
        "2. Créer un Dataset et DataLoader PyTorch.\n",
        "3. Implémenter un cGAN basique (Générateur + Discriminateur conditionnés par texte).\n",
        "4. Entraîner le modèle sur GPU si disponible.\n",
        "5. Générer et visualiser des samples.\n",
        "\n",
        "**Prérequis** :\n",
        "- Installez : `pip install torch torchvision torchtext scikit-learn tqdm`.\n",
        "- Utilisez un GPU (Colab recommandé : Runtime > Change runtime type > GPU).\n",
        "- Fichier `iam_lines_pairs.npz` doit exister (généré dans le notebook précédent).\n",
        "\n",
        "Exécutez cellule par cellule. Temps : ~quelques minutes pour demo, heures pour full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.5.1+cu121\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: C:\\Users\\Hp\\Desktop\\GEN AI\\py310env\\Lib\\site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: torchaudio, torchtext, torchvision\n",
            "---\n",
            "Name: torchtext\n",
            "Version: 0.18.0\n",
            "Summary: Text utilities, models, transforms, and datasets for PyTorch.\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch Text Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD\n",
            "Location: C:\\Users\\Hp\\Desktop\\GEN AI\\py310env\\Lib\\site-packages\n",
            "Requires: numpy, requests, torch, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torchtext in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torchtext) (2.32.5)\n",
            "Requirement already satisfied: torch>=2.3.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torchtext) (2.8.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torchtext) (2.3.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch>=2.3.0->torchtext) (2025.9.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from requests->torchtext) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from requests->torchtext) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from requests->torchtext) (2025.10.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchtext\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
            "     ---------------------------------------- 8.9/8.9 MB 7.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
            "Collecting scipy>=1.8.0\n",
            "  Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl (38.7 MB)\n",
            "     ---------------------------------------- 38.7/38.7 MB 4.6 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "     -------------------------------------- 308.4/308.4 kB 2.7 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports nécessaires\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Device (GPU si disponible)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Chemins\n",
        "PAIRS_NPZ = 'C:\\\\Users\\\\Hp\\\\Desktop\\\\GEN AI\\\\iam_lines_pairs.npz'  # Ajustez si nécessaire\n",
        "TRAIN_CSV = 'iam_train.csv'\n",
        "VAL_CSV = 'iam_val.csv'\n",
        "TEST_CSV = 'iam_test.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 1 : Chargement des paires préparées et création de splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre total de lignes trouvées : 13353\n",
            "✅ Fichier iam_lines_pairs.npz recréé avec succès !\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "# Dossier contenant les fichiers XML\n",
        "xml_dir = r\"C:\\Users\\Hp\\Desktop\\GEN AI\\IAM_dataset\\xml\"\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for file in os.listdir(xml_dir):\n",
        "    if file.endswith(\".xml\"):\n",
        "        xml_path = os.path.join(xml_dir, file)\n",
        "        try:\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            form_id = file.replace(\".xml\", \"\")\n",
        "\n",
        "            # Cherche les lignes manuscrites\n",
        "            for line in root.findall(\".//line\"):\n",
        "                line_id = line.get(\"id\")\n",
        "                text = line.get(\"text\")\n",
        "                if text:\n",
        "                    pairs.append({\n",
        "                        \"id\": line_id,\n",
        "                        \"text\": text,\n",
        "                        \"form_id\": form_id\n",
        "                    })\n",
        "        except ET.ParseError:\n",
        "            print(f\"Erreur de parsing XML pour {xml_path}\")\n",
        "\n",
        "print(f\"Nombre total de lignes trouvées : {len(pairs)}\")\n",
        "\n",
        "# Sauvegarder le fichier .npz\n",
        "np.savez(\"iam_lines_pairs.npz\", pairs=pairs)\n",
        "print(\"✅ Fichier iam_lines_pairs.npz recréé avec succès !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de paires : 19\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNombre de paires : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 3️⃣  Extraire les textes\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m texts = \u001b[43m[\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 4️⃣  Créer le vocabulaire\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     47\u001b[39m vocab, char_to_idx, idx_to_char, max_text_len = create_vocab(texts)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNombre de paires : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 3️⃣  Extraire les textes\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m texts = [\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m pairs]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 4️⃣  Créer le vocabulaire\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m     47\u001b[39m vocab, char_to_idx, idx_to_char, max_text_len = create_vocab(texts)\n",
            "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
          ]
        }
      ],
      "source": [
        "# Tokenisation char-level manuelle\n",
        "def create_vocab(texts):\n",
        "    all_chars = set(''.join(texts))\n",
        "    vocab = ['<pad>', '<unk>'] + sorted(all_chars)\n",
        "    vocab_size = len(vocab)\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "    idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "    max_text_len = max(len(t) for t in texts)  # Pour padding\n",
        "    return vocab, char_to_idx, idx_to_char, max_text_len\n",
        "\n",
        "# Charger les textes\n",
        "data = np.load(PAIRS_NPZ, allow_pickle=True)['pairs']\n",
        "pairs = list(data)\n",
        "texts = [pair['text'] for pair in pairs]\n",
        "\n",
        "# Créer vocabulaire\n",
        "vocab, char_to_idx, idx_to_char, max_text_len = create_vocab(texts)\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocab size: {vocab_size}, Max text len: {max_text_len}')\n",
        "\n",
        "# Fonction pour encoder le texte\n",
        "def encode_text(text):\n",
        "    encoded = [char_to_idx.get(c, char_to_idx['<unk>']) for c in text]\n",
        "    padded = encoded + [char_to_idx['<pad>']] * (max_text_len - len(encoded))\n",
        "    return torch.tensor(padded, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 2 : Tokenisation du texte et création d'un vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 81, Max text len: 97\n"
          ]
        }
      ],
      "source": [
        "# Tous les textes\n",
        "texts = [pair['text'] for pair in pairs]\n",
        "\n",
        "# Vocabulaire char-level\n",
        "all_chars = set(''.join(texts))\n",
        "vocab = ['<pad>', '<unk>'] + sorted(all_chars)\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "max_text_len = max(len(t) for t in texts)  # Pour padding\n",
        "print(f'Vocab size: {vocab_size}, Max text len: {max_text_len}')\n",
        "\n",
        "# Fonction pour encoder le texte\n",
        "def encode_text(text):\n",
        "    encoded = [char_to_idx.get(c, char_to_idx['<unk>']) for c in text]\n",
        "    padded = encoded + [char_to_idx['<pad>']] * (max_text_len - len(encoded))\n",
        "    return torch.tensor(padded, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 3 : Custom Dataset PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 9347  |  Val: 2003  |  Test: 2003\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données\n",
        "data = np.load(\"iam_lines_pairs.npz\", allow_pickle=True)['pairs']\n",
        "\n",
        "# Division en ensembles train / val / test\n",
        "pairs_train, pairs_temp = train_test_split(data, test_size=0.3, random_state=42)\n",
        "pairs_val, pairs_test = train_test_split(pairs_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(pairs_train)}  |  Val: {len(pairs_val)}  |  Test: {len(pairs_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIAMHandwritingDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pairs):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mself\u001b[39m.pairs = pairs\n",
            "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "class IAMHandwritingDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        text = pair['text']\n",
        "        image = pair['image']  # Déjà un tenseur (1, H, W)\n",
        "        text_encoded = encode_text(text)\n",
        "        return {'text': text_encoded, 'image': image, 'raw_text': text}\n",
        "\n",
        "# Création des datasets et dataloaders\n",
        "dataset_train = IAMHandwritingDataset(pairs_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "if len(pairs_val) > 0:  # Remplacez if pairs_val: par une vérification de longueur\n",
        "    dataset_val = IAMHandwritingDataset(pairs_val)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False)\n",
        "if len(pairs_test) > 0:  # Remplacez if pairs_test:\n",
        "    dataset_test = IAMHandwritingDataset(pairs_test)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 4 : Définition du Modèle cGAN Simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting torch==2.5.1\n",
            "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.5.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.5.1) (4.14.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.5.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.5.1) (2025.7.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
            "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
            "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 2.6/203.1 MB 12.6 MB/s eta 0:00:16\n",
            "   - -------------------------------------- 9.2/203.1 MB 22.0 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 13.6/203.1 MB 22.0 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 17.3/203.1 MB 21.0 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 19.9/203.1 MB 18.5 MB/s eta 0:00:10\n",
            "   ---- ----------------------------------- 22.5/203.1 MB 17.2 MB/s eta 0:00:11\n",
            "   ---- ----------------------------------- 24.6/203.1 MB 16.1 MB/s eta 0:00:12\n",
            "   ----- ---------------------------------- 26.7/203.1 MB 15.3 MB/s eta 0:00:12\n",
            "   ----- ---------------------------------- 29.1/203.1 MB 14.8 MB/s eta 0:00:12\n",
            "   ------ --------------------------------- 30.7/203.1 MB 14.1 MB/s eta 0:00:13\n",
            "   ------ --------------------------------- 32.5/203.1 MB 13.5 MB/s eta 0:00:13\n",
            "   ------ --------------------------------- 34.1/203.1 MB 13.0 MB/s eta 0:00:14\n",
            "   ------- -------------------------------- 35.7/203.1 MB 12.6 MB/s eta 0:00:14\n",
            "   ------- -------------------------------- 37.0/203.1 MB 12.2 MB/s eta 0:00:14\n",
            "   ------- -------------------------------- 38.5/203.1 MB 11.9 MB/s eta 0:00:14\n",
            "   ------- -------------------------------- 40.1/203.1 MB 11.7 MB/s eta 0:00:14\n",
            "   -------- ------------------------------- 41.4/203.1 MB 11.1 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 42.7/203.1 MB 10.9 MB/s eta 0:00:15\n",
            "   -------- ------------------------------- 44.3/203.1 MB 10.7 MB/s eta 0:00:15\n",
            "   --------- ------------------------------ 46.1/203.1 MB 10.6 MB/s eta 0:00:15\n",
            "   --------- ------------------------------ 48.0/203.1 MB 10.5 MB/s eta 0:00:15\n",
            "   --------- ------------------------------ 49.8/203.1 MB 10.4 MB/s eta 0:00:15\n",
            "   ---------- ----------------------------- 51.9/203.1 MB 10.4 MB/s eta 0:00:15\n",
            "   ---------- ----------------------------- 54.0/203.1 MB 10.3 MB/s eta 0:00:15\n",
            "   ---------- ----------------------------- 55.8/203.1 MB 10.3 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 57.4/203.1 MB 10.2 MB/s eta 0:00:15\n",
            "   ----------- ---------------------------- 59.5/203.1 MB 10.1 MB/s eta 0:00:15\n",
            "   ------------ --------------------------- 61.3/203.1 MB 10.1 MB/s eta 0:00:14\n",
            "   ------------ --------------------------- 63.4/203.1 MB 10.0 MB/s eta 0:00:14\n",
            "   ------------ --------------------------- 64.7/203.1 MB 9.9 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 66.1/203.1 MB 9.8 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 67.4/203.1 MB 9.7 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 68.9/203.1 MB 9.6 MB/s eta 0:00:14\n",
            "   ------------- -------------------------- 70.3/203.1 MB 9.5 MB/s eta 0:00:14\n",
            "   -------------- ------------------------- 71.6/203.1 MB 9.4 MB/s eta 0:00:14\n",
            "   -------------- ------------------------- 72.6/203.1 MB 9.4 MB/s eta 0:00:14\n",
            "   -------------- ------------------------- 73.9/203.1 MB 9.2 MB/s eta 0:00:14\n",
            "   -------------- ------------------------- 75.0/203.1 MB 9.2 MB/s eta 0:00:14\n",
            "   --------------- ------------------------ 76.3/203.1 MB 9.1 MB/s eta 0:00:14\n",
            "   --------------- ------------------------ 77.6/203.1 MB 9.0 MB/s eta 0:00:14\n",
            "   --------------- ------------------------ 79.2/203.1 MB 8.9 MB/s eta 0:00:14\n",
            "   --------------- ------------------------ 80.5/203.1 MB 8.9 MB/s eta 0:00:14\n",
            "   ---------------- ----------------------- 81.8/203.1 MB 8.8 MB/s eta 0:00:14\n",
            "   ---------------- ----------------------- 83.1/203.1 MB 8.8 MB/s eta 0:00:14\n",
            "   ---------------- ----------------------- 84.4/203.1 MB 8.7 MB/s eta 0:00:14\n",
            "   ---------------- ----------------------- 86.0/203.1 MB 8.7 MB/s eta 0:00:14\n",
            "   ----------------- ---------------------- 87.3/203.1 MB 8.6 MB/s eta 0:00:14\n",
            "   ----------------- ---------------------- 88.6/203.1 MB 8.6 MB/s eta 0:00:14\n",
            "   ----------------- ---------------------- 90.2/203.1 MB 8.5 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 91.5/203.1 MB 8.5 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 92.5/203.1 MB 8.4 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 93.6/203.1 MB 8.4 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 94.6/203.1 MB 8.3 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 95.9/203.1 MB 8.2 MB/s eta 0:00:14\n",
            "   ------------------- -------------------- 97.0/203.1 MB 8.2 MB/s eta 0:00:13\n",
            "   ------------------- -------------------- 97.8/203.1 MB 8.1 MB/s eta 0:00:13\n",
            "   ------------------- -------------------- 98.6/203.1 MB 8.0 MB/s eta 0:00:14\n",
            "   ------------------- -------------------- 99.4/203.1 MB 8.0 MB/s eta 0:00:14\n",
            "   ------------------- -------------------- 100.4/203.1 MB 7.9 MB/s eta 0:00:14\n",
            "   ------------------- -------------------- 101.2/203.1 MB 7.8 MB/s eta 0:00:14\n",
            "   -------------------- ------------------- 102.0/203.1 MB 7.8 MB/s eta 0:00:14\n",
            "   -------------------- ------------------- 103.0/203.1 MB 7.7 MB/s eta 0:00:13\n",
            "   -------------------- ------------------- 104.1/203.1 MB 7.7 MB/s eta 0:00:13\n",
            "   -------------------- ------------------- 105.1/203.1 MB 7.6 MB/s eta 0:00:13\n",
            "   -------------------- ------------------- 106.2/203.1 MB 7.6 MB/s eta 0:00:13\n",
            "   --------------------- ------------------ 107.2/203.1 MB 7.5 MB/s eta 0:00:13\n",
            "   --------------------- ------------------ 108.3/203.1 MB 7.5 MB/s eta 0:00:13\n",
            "   --------------------- ------------------ 109.3/203.1 MB 7.4 MB/s eta 0:00:13\n",
            "   --------------------- ------------------ 110.1/203.1 MB 7.4 MB/s eta 0:00:13\n",
            "   --------------------- ------------------ 111.4/203.1 MB 7.4 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 112.2/203.1 MB 7.3 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 113.0/203.1 MB 7.3 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 113.8/203.1 MB 7.2 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 114.6/203.1 MB 7.2 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 115.3/203.1 MB 7.1 MB/s eta 0:00:13\n",
            "   ---------------------- ----------------- 116.1/203.1 MB 7.1 MB/s eta 0:00:13\n",
            "   ----------------------- ---------------- 117.2/203.1 MB 7.0 MB/s eta 0:00:13\n",
            "   ----------------------- ---------------- 118.0/203.1 MB 7.0 MB/s eta 0:00:13\n",
            "   ----------------------- ---------------- 119.0/203.1 MB 6.9 MB/s eta 0:00:13\n",
            "   ----------------------- ---------------- 120.1/203.1 MB 6.9 MB/s eta 0:00:12\n",
            "   ----------------------- ---------------- 121.1/203.1 MB 6.9 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 122.2/203.1 MB 6.9 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 123.5/203.1 MB 6.9 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 124.3/203.1 MB 6.8 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 125.0/203.1 MB 6.8 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 125.8/203.1 MB 6.7 MB/s eta 0:00:12\n",
            "   ------------------------ --------------- 126.6/203.1 MB 6.7 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 127.7/203.1 MB 6.6 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 128.5/203.1 MB 6.6 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 129.5/203.1 MB 6.5 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 130.5/203.1 MB 6.5 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 131.6/203.1 MB 6.5 MB/s eta 0:00:12\n",
            "   -------------------------- ------------- 132.1/203.1 MB 6.4 MB/s eta 0:00:12\n",
            "   -------------------------- ------------- 133.2/203.1 MB 6.4 MB/s eta 0:00:11\n",
            "   -------------------------- ------------- 134.2/203.1 MB 6.4 MB/s eta 0:00:11\n",
            "   -------------------------- ------------- 135.5/203.1 MB 6.4 MB/s eta 0:00:11\n",
            "   -------------------------- ------------- 136.8/203.1 MB 6.4 MB/s eta 0:00:11\n",
            "   --------------------------- ------------ 138.1/203.1 MB 6.4 MB/s eta 0:00:11\n",
            "   --------------------------- ------------ 139.5/203.1 MB 6.4 MB/s eta 0:00:10\n",
            "   --------------------------- ------------ 141.0/203.1 MB 6.4 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 142.3/203.1 MB 6.4 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 143.7/203.1 MB 6.4 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 145.0/203.1 MB 6.4 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 146.5/203.1 MB 6.4 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 147.8/203.1 MB 6.4 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 149.4/203.1 MB 6.4 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 150.7/203.1 MB 6.4 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 152.0/203.1 MB 6.4 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 153.4/203.1 MB 6.4 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 154.9/203.1 MB 6.4 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 156.2/203.1 MB 6.4 MB/s eta 0:00:08\n",
            "   ------------------------------- -------- 157.5/203.1 MB 6.4 MB/s eta 0:00:08\n",
            "   ------------------------------- -------- 158.9/203.1 MB 6.4 MB/s eta 0:00:07\n",
            "   ------------------------------- -------- 160.4/203.1 MB 6.4 MB/s eta 0:00:07\n",
            "   ------------------------------- -------- 161.0/203.1 MB 6.4 MB/s eta 0:00:07\n",
            "   ------------------------------- -------- 162.3/203.1 MB 6.4 MB/s eta 0:00:07\n",
            "   -------------------------------- ------- 163.6/203.1 MB 6.4 MB/s eta 0:00:07\n",
            "   -------------------------------- ------- 165.2/203.1 MB 6.4 MB/s eta 0:00:06\n",
            "   -------------------------------- ------- 166.7/203.1 MB 6.4 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 168.3/203.1 MB 6.4 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 169.6/203.1 MB 6.4 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 171.2/203.1 MB 6.4 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 172.5/203.1 MB 6.4 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 173.8/203.1 MB 6.4 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 174.6/203.1 MB 6.4 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 175.6/203.1 MB 6.3 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 177.2/203.1 MB 6.3 MB/s eta 0:00:05\n",
            "   ----------------------------------- ---- 178.8/203.1 MB 6.3 MB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 180.4/203.1 MB 6.4 MB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 181.4/203.1 MB 6.3 MB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 182.5/203.1 MB 6.3 MB/s eta 0:00:04\n",
            "   ------------------------------------ --- 183.5/203.1 MB 6.3 MB/s eta 0:00:04\n",
            "   ------------------------------------ --- 184.8/203.1 MB 6.3 MB/s eta 0:00:03\n",
            "   ------------------------------------ --- 186.1/203.1 MB 6.3 MB/s eta 0:00:03\n",
            "   ------------------------------------ --- 187.2/203.1 MB 6.3 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 188.7/203.1 MB 6.3 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 190.1/203.1 MB 6.3 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 191.4/203.1 MB 6.1 MB/s eta 0:00:02\n",
            "   ------------------------------------- -- 192.7/203.1 MB 6.0 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 194.0/203.1 MB 5.9 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 195.6/203.1 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 196.9/203.1 MB 5.8 MB/s eta 0:00:02\n",
            "   ---------------------------------------  198.4/203.1 MB 5.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  200.0/203.1 MB 5.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  201.9/203.1 MB 5.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 203.1/203.1 MB 5.7 MB/s  0:00:32\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 1.3/6.2 MB 8.4 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 2.6/6.2 MB 6.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 4.2/6.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 5.8/6.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.2/6.2 MB 6.8 MB/s  0:00:00\n",
            "Installing collected packages: sympy, torch\n",
            "\n",
            "  Attempting uninstall: sympy\n",
            "\n",
            "    Found existing installation: sympy 1.14.0\n",
            "\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "    Uninstalling sympy-1.14.0:\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "  Attempting uninstall: torch\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "    Found existing installation: torch 2.2.0\n",
            "   ---------------------------------------- 0/2 [sympy]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "    Uninstalling torch-2.2.0:\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "      Successfully uninstalled torch-2.2.0\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0 requires torch==2.8.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchtext 0.17.0 requires torch==2.2.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.23.0 requires torch==2.8.0, but you have torch 2.5.1 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torch]\n",
            "   ---------------------------------------- 2/2 [torch]\n",
            "\n",
            "Successfully installed sympy-1.13.1 torch-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.5.1 --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.tensor([1.0], device='cpu')\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_text_len = 20\n",
        "device = 'cpu'  # ou 'cuda' si GPU dispo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n",
            "WARNING: Skipping torchvision as it is not installed.\n",
            "WARNING: Skipping torchaudio as it is not installed.\n",
            "ERROR: pip cache commands can not function since cache is disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.ngc.nvidia.com\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-win_amd64.whl (2449.4 MB)\n",
            "     ---------------------------------------- 2.4/2.4 GB 18.7 MB/s eta 0:00:00\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-win_amd64.whl (6.1 MB)\n",
            "     ---------------------------------------- 6.1/6.1 MB 32.2 MB/s eta 0:00:00\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-win_amd64.whl (4.1 MB)\n",
            "     ---------------------------------------- 4.1/4.1 MB 29.3 MB/s eta 0:00:00\n",
            "Collecting filelock\n",
            "  Obtaining dependency information for filelock from https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\desktop\\gen ai\\py310env\\lib\\site-packages (from torch) (4.15.0)\n",
            "Collecting networkx\n",
            "  Obtaining dependency information for networkx from https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2\n",
            "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Discarding https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from https://download.pytorch.org/whl/cu121/jinja2/): Requested jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\n",
            "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Discarding https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from https://download.pytorch.org/whl/cu121/jinja2/): Requested jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "     ---------------------------------------- 133.2/133.2 kB ? eta 0:00:00\n",
            "Collecting fsspec\n",
            "  Obtaining dependency information for fsspec from https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sympy==1.13.1\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "     ---------------------------------------- 6.2/6.2 MB 30.4 MB/s eta 0:00:00\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "     -------------------------------------- 536.2/536.2 kB 1.1 MB/s eta 0:00:00\n",
            "Collecting numpy\n",
            "  Obtaining dependency information for numpy from https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 60.9/60.9 kB ? eta 0:00:00\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
            "Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
            "   ---------------------------------------- 7.0/7.0 MB 20.3 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "   --------------------------------------- 199.3/199.3 kB 11.8 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "   ---------------------------------------- 2.0/2.0 MB 10.0 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
            "   ---------------------------------------- 13.1/13.1 MB 17.2 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
            "   ---------------------------------------- 7.0/7.0 MB 10.4 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "   --------------------------------------- 199.3/199.3 kB 11.8 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "   ---------------------------------------- 2.0/2.0 MB 14.4 MB/s eta 0:00:00\n",
            "Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
            "   ---------------------------------------- 13.1/13.1 MB 13.1 MB/s eta 0:00:00\n",
            "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
            "Successfully installed MarkupSafe-2.1.5 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 pillow-11.3.0 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: There was an error checking the latest version of pip.\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip cache purge\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu, https://pypi.ngc.nvidia.com\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.4.0%2Bcpu-cp311-cp311-win_amd64.whl (200.2 MB)\n",
            "     ---------------------------------------- 0.0/200.2 MB ? eta -:--:--\n",
            "     ---------------------------------------- 1.6/200.2 MB 8.4 MB/s eta 0:00:24\n",
            "     - ------------------------------------ 10.0/200.2 MB 28.3 MB/s eta 0:00:07\n",
            "     - ------------------------------------ 10.5/200.2 MB 19.3 MB/s eta 0:00:10\n",
            "     --- ---------------------------------- 21.0/200.2 MB 26.5 MB/s eta 0:00:07\n",
            "     ----- -------------------------------- 28.8/200.2 MB 30.5 MB/s eta 0:00:06\n",
            "     --------- ---------------------------- 50.3/200.2 MB 41.1 MB/s eta 0:00:04\n",
            "     ------------ ------------------------- 68.2/200.2 MB 47.7 MB/s eta 0:00:03\n",
            "     ---------------- --------------------- 86.8/200.2 MB 53.8 MB/s eta 0:00:03\n",
            "     ------------------ ------------------- 98.8/200.2 MB 53.9 MB/s eta 0:00:02\n",
            "     -------------------- ---------------- 112.2/200.2 MB 56.0 MB/s eta 0:00:02\n",
            "     ---------------------- -------------- 120.6/200.2 MB 53.5 MB/s eta 0:00:02\n",
            "     ------------------------ ------------ 132.9/200.2 MB 56.2 MB/s eta 0:00:02\n",
            "     ---------------------------- -------- 153.1/200.2 MB 57.2 MB/s eta 0:00:01\n",
            "     ------------------------------ ------ 166.5/200.2 MB 57.5 MB/s eta 0:00:01\n",
            "     --------------------------------- --- 179.0/200.2 MB 58.4 MB/s eta 0:00:01\n",
            "     ------------------------------------  197.7/200.2 MB 59.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 200.2/200.2 MB 59.0 MB/s  0:00:03\n",
            "Collecting torchvision==0.15.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.15.1%2Bcpu-cp311-cp311-win_amd64.whl (1.2 MB)\n",
            "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "     ---------------------------------------- 1.2/1.2 MB 58.0 MB/s  0:00:00\n",
            "Collecting torchaudio==2.4.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.1%2Bcpu-cp311-cp311-win_amd64.whl (2.4 MB)\n",
            "     ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "     ---------------------------------------- 2.4/2.4 MB 15.2 MB/s  0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.4.0) (2025.7.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision==0.15.1) (2.3.2)\n",
            "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision==0.15.1) (2.32.4)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested torch==2.4.0\n",
            "    torchvision 0.15.1+cpu depends on torch==2.0.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Cannot install torch==2.4.0 and torchvision==0.15.1+cpu because these package versions have conflicting dependencies.\n",
            "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.4.0 torchvision==0.15.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam fonctionne !\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "linear = nn.Linear(10, 1)\n",
        "opt = optim.Adam(linear.parameters())\n",
        "x = torch.randn(1, 10)\n",
        "loss = linear(x).sum()\n",
        "loss.backward()\n",
        "opt.step()\n",
        "print(\"Adam fonctionne !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vocab_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.joint_model(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Instanciation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m gen = \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     59\u001b[39m disc = Discriminator().to(device)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Optimizers\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mGenerator.__init__\u001b[39m\u001b[34m(self, embed_dim, noise_dim)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim=\u001b[32m256\u001b[39m, noise_dim=\u001b[32m100\u001b[39m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28mself\u001b[39m.text_embed = TextEmbedding(\u001b[43mvocab_size\u001b[49m, embed_dim)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = nn.Sequential(\n\u001b[32m     16\u001b[39m         nn.Linear(max_text_len * embed_dim + noise_dim, \u001b[32m128\u001b[39m * \u001b[32m16\u001b[39m * \u001b[32m4\u001b[39m),\n\u001b[32m     17\u001b[39m         nn.ReLU(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m         nn.Tanh()\n\u001b[32m     25\u001b[39m     )\n",
            "\u001b[31mNameError\u001b[39m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "# Embedding pour texte\n",
        "class TextEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        return self.embedding(text)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "# Générateur simple\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, embed_dim=256, noise_dim=100):\n",
        "        super().__init__()\n",
        "        self.text_embed = TextEmbedding(vocab_size, embed_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(max_text_len * embed_dim + noise_dim, 128 * 16 * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 16, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, text, noise):\n",
        "        text_emb = self.text_embed(text).view(text.size(0), -1)\n",
        "        input = torch.cat([text_emb, noise], dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "# Discriminateur\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.text_embed = TextEmbedding(vocab_size, embed_dim)\n",
        "        self.img_model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.joint_model = nn.Sequential(\n",
        "            nn.Conv2d(64 + embed_dim, 128, kernel_size=3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, kernel_size=3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, image, text):\n",
        "        img_feat = self.img_model(image)\n",
        "        text_emb = self.text_embed(text).max(dim=1)[0].unsqueeze(2).unsqueeze(3)\n",
        "        text_emb = text_emb.expand(-1, -1, img_feat.size(2), img_feat.size(3))\n",
        "        input = torch.cat([img_feat, text_emb], dim=1)\n",
        "        return self.joint_model(input)\n",
        "\n",
        "# Instanciation\n",
        "gen = Generator().to(device)\n",
        "disc = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "gen_opt = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "disc_opt = optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 5 : Boucle d'Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 5  # Augmentez à 50+ pour un entraînement réel\n",
        "noise_dim = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    total_disc_loss = 0\n",
        "    total_gen_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch in tqdm(dataloader_train):\n",
        "        images = batch['image'].to(device)  # (batch, 1, H, W)\n",
        "        texts = batch['text'].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        \n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "        \n",
        "        # Train Discriminateur\n",
        "        disc_opt.zero_grad()\n",
        "        real_pred = disc(images, texts)\n",
        "        disc_real_loss = criterion(real_pred, real_labels)\n",
        "        \n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_images = gen(texts, noise)\n",
        "        fake_pred = disc(fake_images.detach(), texts)\n",
        "        disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "        \n",
        "        disc_loss = disc_real_loss + disc_fake_loss\n",
        "        disc_loss.backward()\n",
        "        disc_opt.step()\n",
        "        \n",
        "        # Train Générateur\n",
        "        gen_opt.zero_grad()\n",
        "        fake_pred = disc(fake_images, texts)\n",
        "        gen_loss = criterion(fake_pred, real_labels)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        total_disc_loss += disc_loss.item()\n",
        "        total_gen_loss += gen_loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_disc_loss = total_disc_loss / num_batches\n",
        "    avg_gen_loss = total_gen_loss / num_batches\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Disc Loss: {avg_disc_loss:.4f}, Gen Loss: {avg_gen_loss:.4f}')\n",
        "    \n",
        "    # Sauvegarde du modèle\n",
        "    torch.save(gen.state_dict(), f'gen_epoch_{epoch}.pth')\n",
        "    torch.save(disc.state_dict(), f'disc_epoch_{epoch}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 6 : Génération et Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference\n",
        "gen.eval()\n",
        "test_text = \"Hello world\"  # Texte personnalisé\n",
        "test_encoded = encode_text(test_text).unsqueeze(0).to(device)\n",
        "noise = torch.randn(1, noise_dim).to(device)\n",
        "with torch.no_grad():\n",
        "    fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "\n",
        "# Ajuster la normalisation (inverser Tanh et Normalize)\n",
        "fake_img = (fake_img + 1) / 2  # De [-1, 1] à [0, 1]\n",
        "plt.imshow(fake_img[0], cmap='gray')\n",
        "plt.title(f'Generated: {test_text}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boucle d'Entraînement Améliorée\n",
        "num_epochs = 20  # Augmenté pour de meilleurs résultats\n",
        "noise_dim = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    total_disc_loss = 0\n",
        "    total_gen_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Entraînement\n",
        "    for batch in tqdm(dataloader_train, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):\n",
        "        images = batch['image'].to(device)\n",
        "        texts = batch['text'].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        \n",
        "        real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "        \n",
        "        # Discriminateur\n",
        "        disc_opt.zero_grad()\n",
        "        real_pred = disc(images, texts)\n",
        "        disc_real_loss = criterion(real_pred, real_labels)\n",
        "        \n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_images = gen(texts, noise)\n",
        "        fake_pred = disc(fake_images.detach(), texts)\n",
        "        disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "        \n",
        "        disc_loss = disc_real_loss + disc_fake_loss\n",
        "        disc_loss.backward()\n",
        "        disc_opt.step()\n",
        "        \n",
        "        # Générateur\n",
        "        gen_opt.zero_grad()\n",
        "        fake_pred = disc(fake_images, texts)\n",
        "        gen_loss = criterion(fake_pred, real_labels)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        total_disc_loss += disc_loss.item()\n",
        "        total_gen_loss += gen_loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_disc_loss = total_disc_loss / num_batches\n",
        "    avg_gen_loss = total_gen_loss / num_batches\n",
        "    \n",
        "    # Validation (si des données de validation existent)\n",
        "    if dataloader_val:\n",
        "        gen.eval()\n",
        "        total_val_disc_loss = 0\n",
        "        total_val_gen_loss = 0\n",
        "        val_num_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader_val:\n",
        "                images = batch['image'].to(device)\n",
        "                texts = batch['text'].to(device)\n",
        "                batch_size = images.size(0)\n",
        "                \n",
        "                real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "                fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "                \n",
        "                real_pred = disc(images, texts)\n",
        "                val_disc_real_loss = criterion(real_pred, real_labels)\n",
        "                \n",
        "                noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "                fake_images = gen(texts, noise)\n",
        "                fake_pred = disc(fake_images, texts)\n",
        "                val_disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "                \n",
        "                val_disc_loss = val_disc_real_loss + val_disc_fake_loss\n",
        "                val_gen_loss = criterion(fake_pred, real_labels)\n",
        "                \n",
        "                total_val_disc_loss += val_disc_loss.item()\n",
        "                total_val_gen_loss += val_gen_loss.item()\n",
        "                val_num_batches += 1\n",
        "        \n",
        "        avg_val_disc_loss = total_val_disc_loss / val_num_batches\n",
        "        avg_val_gen_loss = total_val_gen_loss / val_num_batches\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Disc Loss: {avg_disc_loss:.4f}, Train Gen Loss: {avg_gen_loss:.4f}, '\n",
        "              f'Val Disc Loss: {avg_val_disc_loss:.4f}, Val Gen Loss: {avg_val_gen_loss:.4f}')\n",
        "    else:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Disc Loss: {avg_disc_loss:.4f}, Train Gen Loss: {avg_gen_loss:.4f}')\n",
        "    \n",
        "    # Sauvegarde du modèle\n",
        "    torch.save(gen.state_dict(), f'gen_epoch_{epoch}.pth')\n",
        "    torch.save(disc.state_dict(), f'disc_epoch_{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Évaluation et Visualisation\n",
        "gen.eval()\n",
        "num_samples = 5\n",
        "sample_texts = [\"Hello world\", \"This is a test\", \"Machine learning\", \"Handwritten text\", \"Good day!\"]\n",
        "\n",
        "plt.figure(figsize=(15, 3 * num_samples))\n",
        "for i, text in enumerate(sample_texts):\n",
        "    test_encoded = encode_text(text).unsqueeze(0).to(device)\n",
        "    noise = torch.randn(1, noise_dim).to(device)\n",
        "    with torch.no_grad():\n",
        "        fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "    \n",
        "    fake_img = (fake_img + 1) / 2  # De [-1, 1] à [0, 1]\n",
        "    plt.subplot(num_samples, 1, i+1)\n",
        "    plt.imshow(fake_img[0], cmap='gray')\n",
        "    plt.title(f'Generated: {text}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sauvegarde des images (optionnel)\n",
        "for i, text in enumerate(sample_texts):\n",
        "    test_encoded = encode_text(text).unsqueeze(0).to(device)\n",
        "    noise = torch.randn(1, noise_dim).to(device)\n",
        "    with torch.no_grad():\n",
        "        fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "    fake_img = (fake_img + 1) / 2\n",
        "    img = Image.fromarray((fake_img[0] * 255).astype(np.uint8))\n",
        "    img.save(f'generated_{text.replace(\" \", \"_\")}.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
