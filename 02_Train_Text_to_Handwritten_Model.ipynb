{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Étape Suivante : Création d'un Dataset PyTorch et Entraînement Basique d'un Modèle Text-to-Handwritten\n",
        "\n",
        "Ce notebook utilise les paires préparées dans `01_Prepare_IAM_Dataset.ipynb` pour entraîner un modèle cGAN simple. Objectifs :\n",
        "1. Charger les paires (texte tokenisé + images) à partir de `iam_lines_pairs.npz`.\n",
        "2. Créer un Dataset et DataLoader PyTorch.\n",
        "3. Implémenter un cGAN basique (Générateur + Discriminateur conditionnés par texte).\n",
        "4. Entraîner le modèle sur GPU si disponible.\n",
        "5. Générer et visualiser des samples.\n",
        "\n",
        "**Prérequis** :\n",
        "- Installez : `pip install torch torchvision torchtext scikit-learn tqdm`.\n",
        "- Utilisez un GPU (Colab recommandé : Runtime > Change runtime type > GPU).\n",
        "- Fichier `iam_lines_pairs.npz` doit exister (généré dans le notebook précédent).\n",
        "\n",
        "Exécutez cellule par cellule. Temps : ~quelques minutes pour demo, heures pour full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show torch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!pip install --upgrade torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall torchtext\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchtext==0.17.0  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports nécessaires\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Device (GPU si disponible)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Chemins\n",
        "PAIRS_NPZ = 'C:\\\\Users\\\\Hp\\\\Desktop\\\\GEN AI\\\\iam_lines_pairs.npz'  # Ajustez si nécessaire\n",
        "TRAIN_CSV = 'iam_train.csv'\n",
        "VAL_CSV = 'iam_val.csv'\n",
        "TEST_CSV = 'iam_test.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 1 : Chargement des paires préparées et création de splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Transformation pour les images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 1024)),  # Taille cible unifiée\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normaliser entre -1 et 1\n",
        "])\n",
        "\n",
        "# Dossiers\n",
        "xml_dir = r\"C:\\Users\\Hp\\Desktop\\GEN AI\\IAM_dataset\\xml\"\n",
        "img_dir = r\"C:\\Users\\Hp\\Desktop\\GEN AI\\IAM_dataset\\lines\"\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for file in os.listdir(xml_dir):\n",
        "    if file.endswith(\".xml\"):\n",
        "        xml_path = os.path.join(xml_dir, file)\n",
        "        try:\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            form_id = file.replace(\".xml\", \"\")\n",
        "\n",
        "            for line in root.findall(\".//line\"):\n",
        "                line_id = line.get(\"id\")\n",
        "                text = line.get(\"text\")\n",
        "                if text:\n",
        "                    # Construire le chemin de l'image\n",
        "                    parts = line_id.split('-')\n",
        "                    img_path = os.path.join(img_dir, parts[0], f\"{parts[0]}-{parts[1]}\", f\"{line_id}.png\")\n",
        "\n",
        "                    if os.path.exists(img_path):\n",
        "                        try:\n",
        "                            image = Image.open(img_path).convert(\"L\")\n",
        "                            image_tensor = transform(image)\n",
        "                            pairs.append({\n",
        "                                \"id\": line_id,\n",
        "                                \"text\": text,\n",
        "                                \"form_id\": form_id,\n",
        "                                \"image\": image_tensor\n",
        "                            })\n",
        "                        except Exception as e:\n",
        "                            print(f\"Erreur de traitement pour l'image {img_path}: {e}\")\n",
        "        except ET.ParseError:\n",
        "            print(f\"Erreur de parsing XML pour {xml_path}\")\n",
        "\n",
        "print(f\"Nombre total de paires (texte, image) trouvées : {len(pairs)}\")\n",
        "\n",
        "# Sauvegarder le fichier .npz\n",
        "np.savez(\"iam_lines_pairs.npz\", pairs=pairs)\n",
        "print(\"✅ Fichier iam_lines_pairs.npz recréé avec succès !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenisation char-level manuelle\n",
        "def create_vocab(texts):\n",
        "    all_chars = set(''.join(texts))\n",
        "    vocab = ['<pad>', '<unk>'] + sorted(all_chars)\n",
        "    vocab_size = len(vocab)\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "    idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "    max_text_len = max(len(t) for t in texts)  # Pour padding\n",
        "    return vocab, char_to_idx, idx_to_char, max_text_len\n",
        "\n",
        "# Charger les textes\n",
        "data = np.load(PAIRS_NPZ, allow_pickle=True)['pairs']\n",
        "pairs = list(data)\n",
        "texts = [pair['text'] for pair in pairs]\n",
        "\n",
        "# Créer vocabulaire\n",
        "vocab, char_to_idx, idx_to_char, max_text_len = create_vocab(texts)\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocab size: {vocab_size}, Max text len: {max_text_len}')\n",
        "\n",
        "# Fonction pour encoder le texte\n",
        "def encode_text(text):\n",
        "    encoded = [char_to_idx.get(c, char_to_idx['<unk>']) for c in text]\n",
        "    padded = encoded + [char_to_idx['<pad>']] * (max_text_len - len(encoded))\n",
        "    return torch.tensor(padded, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 2 : Tokenisation du texte et création d'un vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tous les textes\n",
        "texts = [pair['text'] for pair in pairs]\n",
        "\n",
        "# Vocabulaire char-level\n",
        "all_chars = set(''.join(texts))\n",
        "vocab = ['<pad>', '<unk>'] + sorted(all_chars)\n",
        "vocab_size = len(vocab)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
        "max_text_len = max(len(t) for t in texts)  # Pour padding\n",
        "print(f'Vocab size: {vocab_size}, Max text len: {max_text_len}')\n",
        "\n",
        "# Fonction pour encoder le texte\n",
        "def encode_text(text):\n",
        "    encoded = [char_to_idx.get(c, char_to_idx['<unk>']) for c in text]\n",
        "    padded = encoded + [char_to_idx['<pad>']] * (max_text_len - len(encoded))\n",
        "    return torch.tensor(padded, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 3 : Custom Dataset PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données\n",
        "data = np.load(\"iam_lines_pairs.npz\", allow_pickle=True)['pairs']\n",
        "\n",
        "# Division en ensembles train / val / test\n",
        "pairs_train, pairs_temp = train_test_split(data, test_size=0.3, random_state=42)\n",
        "pairs_val, pairs_test = train_test_split(pairs_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(pairs_train)}  |  Val: {len(pairs_val)}  |  Test: {len(pairs_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IAMHandwritingDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        text = pair['text']\n",
        "        image = pair['image']  # Déjà un tenseur (1, H, W)\n",
        "        text_encoded = encode_text(text)\n",
        "        return {'text': text_encoded, 'image': image, 'raw_text': text}\n",
        "\n",
        "# Création des datasets et dataloaders\n",
        "dataset_train = IAMHandwritingDataset(pairs_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "if len(pairs_val) > 0:  # Remplacez if pairs_val: par une vérification de longueur\n",
        "    dataset_val = IAMHandwritingDataset(pairs_val)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False)\n",
        "if len(pairs_test) > 0:  # Remplacez if pairs_test:\n",
        "    dataset_test = IAMHandwritingDataset(pairs_test)\n",
        "    dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 4 : Définition du Modèle cGAN Simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch==2.5.1 --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.tensor([1.0], device='cpu')\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_text_len = 20\n",
        "device = 'cpu'  # ou 'cuda' si GPU dispo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip cache purge\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch==2.4.0 torchvision==0.15.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "linear = nn.Linear(10, 1)\n",
        "opt = optim.Adam(linear.parameters())\n",
        "x = torch.randn(1, 10)\n",
        "loss = linear(x).sum()\n",
        "loss.backward()\n",
        "opt.step()\n",
        "print(\"Adam fonctionne !\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding pour texte\n",
        "class TextEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        return self.embedding(text)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "# Générateur simple\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, embed_dim=256, noise_dim=100):\n",
        "        super().__init__()\n",
        "        self.text_embed = TextEmbedding(vocab_size, embed_dim)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(max_text_len * embed_dim + noise_dim, 128 * 16 * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 16, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, text, noise):\n",
        "        text_emb = self.text_embed(text).view(text.size(0), -1)\n",
        "        input = torch.cat([text_emb, noise], dim=1)\n",
        "        return self.model(input)\n",
        "\n",
        "# Discriminateur\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.text_embed = TextEmbedding(vocab_size, embed_dim)\n",
        "        self.img_model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.joint_model = nn.Sequential(\n",
        "            nn.Conv2d(64 + embed_dim, 128, kernel_size=3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, kernel_size=3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, image, text):\n",
        "        img_feat = self.img_model(image)\n",
        "        text_emb = self.text_embed(text).max(dim=1)[0].unsqueeze(2).unsqueeze(3)\n",
        "        text_emb = text_emb.expand(-1, -1, img_feat.size(2), img_feat.size(3))\n",
        "        input = torch.cat([img_feat, text_emb], dim=1)\n",
        "        return self.joint_model(input)\n",
        "\n",
        "# Instanciation\n",
        "gen = Generator().to(device)\n",
        "disc = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "gen_opt = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "disc_opt = optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 5 : Boucle d'Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 5  # Augmentez à 50+ pour un entraînement réel\n",
        "noise_dim = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    total_disc_loss = 0\n",
        "    total_gen_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch in tqdm(dataloader_train):\n",
        "        images = batch['image'].to(device)  # (batch, 1, H, W)\n",
        "        texts = batch['text'].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        \n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "        \n",
        "        # Train Discriminateur\n",
        "        disc_opt.zero_grad()\n",
        "        real_pred = disc(images, texts)\n",
        "        disc_real_loss = criterion(real_pred, real_labels)\n",
        "        \n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_images = gen(texts, noise)\n",
        "        fake_pred = disc(fake_images.detach(), texts)\n",
        "        disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "        \n",
        "        disc_loss = disc_real_loss + disc_fake_loss\n",
        "        disc_loss.backward()\n",
        "        disc_opt.step()\n",
        "        \n",
        "        # Train Générateur\n",
        "        gen_opt.zero_grad()\n",
        "        fake_pred = disc(fake_images, texts)\n",
        "        gen_loss = criterion(fake_pred, real_labels)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        total_disc_loss += disc_loss.item()\n",
        "        total_gen_loss += gen_loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_disc_loss = total_disc_loss / num_batches\n",
        "    avg_gen_loss = total_gen_loss / num_batches\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Disc Loss: {avg_disc_loss:.4f}, Gen Loss: {avg_gen_loss:.4f}')\n",
        "    \n",
        "    # Sauvegarde du modèle\n",
        "    torch.save(gen.state_dict(), f'gen_epoch_{epoch}.pth')\n",
        "    torch.save(disc.state_dict(), f'disc_epoch_{epoch}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Étape 6 : Génération et Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference\n",
        "gen.eval()\n",
        "test_text = \"Hello world\"  # Texte personnalisé\n",
        "test_encoded = encode_text(test_text).unsqueeze(0).to(device)\n",
        "noise = torch.randn(1, noise_dim).to(device)\n",
        "with torch.no_grad():\n",
        "    fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "\n",
        "# Ajuster la normalisation (inverser Tanh et Normalize)\n",
        "fake_img = (fake_img + 1) / 2  # De [-1, 1] à [0, 1]\n",
        "plt.imshow(fake_img[0], cmap='gray')\n",
        "plt.title(f'Generated: {test_text}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boucle d'Entraînement Améliorée\n",
        "num_epochs = 20  # Augmenté pour de meilleurs résultats\n",
        "noise_dim = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    total_disc_loss = 0\n",
        "    total_gen_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Entraînement\n",
        "    for batch in tqdm(dataloader_train, desc=f'Epoch {epoch+1}/{num_epochs} - Train'):\n",
        "        images = batch['image'].to(device)\n",
        "        texts = batch['text'].to(device)\n",
        "        batch_size = images.size(0)\n",
        "        \n",
        "        real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "        \n",
        "        # Discriminateur\n",
        "        disc_opt.zero_grad()\n",
        "        real_pred = disc(images, texts)\n",
        "        disc_real_loss = criterion(real_pred, real_labels)\n",
        "        \n",
        "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "        fake_images = gen(texts, noise)\n",
        "        fake_pred = disc(fake_images.detach(), texts)\n",
        "        disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "        \n",
        "        disc_loss = disc_real_loss + disc_fake_loss\n",
        "        disc_loss.backward()\n",
        "        disc_opt.step()\n",
        "        \n",
        "        # Générateur\n",
        "        gen_opt.zero_grad()\n",
        "        fake_pred = disc(fake_images, texts)\n",
        "        gen_loss = criterion(fake_pred, real_labels)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        \n",
        "        total_disc_loss += disc_loss.item()\n",
        "        total_gen_loss += gen_loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_disc_loss = total_disc_loss / num_batches\n",
        "    avg_gen_loss = total_gen_loss / num_batches\n",
        "    \n",
        "    # Validation (si des données de validation existent)\n",
        "    if dataloader_val:\n",
        "        gen.eval()\n",
        "        total_val_disc_loss = 0\n",
        "        total_val_gen_loss = 0\n",
        "        val_num_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader_val:\n",
        "                images = batch['image'].to(device)\n",
        "                texts = batch['text'].to(device)\n",
        "                batch_size = images.size(0)\n",
        "                \n",
        "                real_labels = torch.ones(batch_size, 1, 1, 1).to(device)\n",
        "                fake_labels = torch.zeros(batch_size, 1, 1, 1).to(device)\n",
        "                \n",
        "                real_pred = disc(images, texts)\n",
        "                val_disc_real_loss = criterion(real_pred, real_labels)\n",
        "                \n",
        "                noise = torch.randn(batch_size, noise_dim).to(device)\n",
        "                fake_images = gen(texts, noise)\n",
        "                fake_pred = disc(fake_images, texts)\n",
        "                val_disc_fake_loss = criterion(fake_pred, fake_labels)\n",
        "                \n",
        "                val_disc_loss = val_disc_real_loss + val_disc_fake_loss\n",
        "                val_gen_loss = criterion(fake_pred, real_labels)\n",
        "                \n",
        "                total_val_disc_loss += val_disc_loss.item()\n",
        "                total_val_gen_loss += val_gen_loss.item()\n",
        "                val_num_batches += 1\n",
        "        \n",
        "        avg_val_disc_loss = total_val_disc_loss / val_num_batches\n",
        "        avg_val_gen_loss = total_val_gen_loss / val_num_batches\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Disc Loss: {avg_disc_loss:.4f}, Train Gen Loss: {avg_gen_loss:.4f}, '\n",
        "              f'Val Disc Loss: {avg_val_disc_loss:.4f}, Val Gen Loss: {avg_val_gen_loss:.4f}')\n",
        "    else:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Disc Loss: {avg_disc_loss:.4f}, Train Gen Loss: {avg_gen_loss:.4f}')\n",
        "    \n",
        "    # Sauvegarde du modèle\n",
        "    torch.save(gen.state_dict(), f'gen_epoch_{epoch}.pth')\n",
        "    torch.save(disc.state_dict(), f'disc_epoch_{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Évaluation et Visualisation\n",
        "gen.eval()\n",
        "num_samples = 5\n",
        "sample_texts = [\"Hello world\", \"This is a test\", \"Machine learning\", \"Handwritten text\", \"Good day!\"]\n",
        "\n",
        "plt.figure(figsize=(15, 3 * num_samples))\n",
        "for i, text in enumerate(sample_texts):\n",
        "    test_encoded = encode_text(text).unsqueeze(0).to(device)\n",
        "    noise = torch.randn(1, noise_dim).to(device)\n",
        "    with torch.no_grad():\n",
        "        fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "    \n",
        "    fake_img = (fake_img + 1) / 2  # De [-1, 1] à [0, 1]\n",
        "    plt.subplot(num_samples, 1, i+1)\n",
        "    plt.imshow(fake_img[0], cmap='gray')\n",
        "    plt.title(f'Generated: {text}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sauvegarde des images (optionnel)\n",
        "for i, text in enumerate(sample_texts):\n",
        "    test_encoded = encode_text(text).unsqueeze(0).to(device)\n",
        "    noise = torch.randn(1, noise_dim).to(device)\n",
        "    with torch.no_grad():\n",
        "        fake_img = gen(test_encoded, noise).cpu().squeeze(0).numpy()\n",
        "    fake_img = (fake_img + 1) / 2\n",
        "    img = Image.fromarray((fake_img[0] * 255).astype(np.uint8))\n",
        "    img.save(f'generated_{text.replace(\" \", \"_\")}.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
